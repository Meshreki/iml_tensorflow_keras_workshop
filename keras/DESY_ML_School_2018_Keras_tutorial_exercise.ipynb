{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cifar-10 Classification using Keras Tutorial\n",
    "\n",
    "The CIFAR-10 data set consists of 60000 32×32 color images in 10 classes, with 6000 images per class. There are 50000 training images and 10000 test images.\n",
    "\n",
    "Recognizing photos from the cifar-10 collection is one of the most common problems in the today’s  world of machine learning. I’m going to show you – step by step – how to build multi-layer artificial neural networks that will recognize images from a cifar-10  set with an accuracy of about 80% and visualize it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import all modules\n",
    "import keras\n",
    "from keras.models import Sequential, Model\n",
    "from keras.layers import Input, Dense, Dropout, Flatten, Activation\n",
    "from keras.constraints import maxnorm\n",
    "from keras.optimizers import Adam, SGD\n",
    "from keras.layers import Activation\n",
    "from keras.layers.convolutional import Conv2D\n",
    "from keras.layers.convolutional import MaxPooling2D, ZeroPadding2D\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.utils import np_utils\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.callbacks import ModelCheckpoint, EarlyStopping\n",
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "\n",
    "from keras import backend as K\n",
    "\n",
    "# Loading the CIFAR-10 datasets\n",
    "from keras.datasets import cifar10\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.misc import toimage\n",
    "\n",
    "import cv2\n",
    "from PIL import Image \n",
    "\n",
    "import itertools\n",
    "\n",
    "import numpy as np\n",
    "np.random.seed(1337) # for reproducibility"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set of helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_images(images, labels_true, class_names, labels_pred=None):\n",
    "\n",
    "    assert len(images) == len(labels_true)\n",
    "\n",
    "    # Create a figure with sub-plots\n",
    "    fig, axes = plt.subplots(3, 3, figsize = (8,8))\n",
    "\n",
    "    # Adjust the vertical spacing\n",
    "    if labels_pred is None:\n",
    "        hspace = 0.2\n",
    "    else:\n",
    "        hspace = 0.5\n",
    "    fig.subplots_adjust(hspace=hspace, wspace=0.3)\n",
    "\n",
    "    for i, ax in enumerate(axes.flat):\n",
    "        # Fix crash when less than 9 images\n",
    "        if i < len(images):\n",
    "            # Plot the image\n",
    "            ax.imshow(images[i], interpolation='spline16')\n",
    "            \n",
    "            # Name of the true class\n",
    "            z = 1*labels_true[i].tolist()\n",
    "            labels_true_name = class_names[z.index(max(z))]\n",
    "\n",
    "            # Show true and predicted classes\n",
    "            if labels_pred is None:\n",
    "                xlabel = \"True: \"+labels_true_name\n",
    "            else:\n",
    "                # Name of the predicted class\n",
    "                labels_pred_name = class_names[labels_pred[i]]\n",
    "\n",
    "                xlabel = \"True: \"+labels_true_name+\"\\nPredicted: \"+ labels_pred_name\n",
    "\n",
    "            # Show the class on the x-axis\n",
    "            ax.set_xlabel(xlabel)\n",
    "        \n",
    "        # Remove ticks from the plot\n",
    "        ax.set_xticks([])\n",
    "        ax.set_yticks([])\n",
    "    \n",
    "    # Show the plot\n",
    "    return plt.show()\n",
    "\n",
    "def plot_model(model_details):\n",
    "\n",
    "    # Create sub-plots\n",
    "    fig, axs = plt.subplots(1,2,figsize=(15,5))\n",
    "    \n",
    "    # Set background to white\n",
    "    fig.patch.set_facecolor('white')\n",
    "    axs[0].patch.set_facecolor('white')\n",
    "    axs[1].patch.set_facecolor('white')\n",
    "    \n",
    "    # Don't allow the axis to be on top of your data\n",
    "    axs[0].set_axisbelow(True)\n",
    "    axs[1].set_axisbelow(True)\n",
    "    \n",
    "    # Customize the grid\n",
    "    axs[0].grid(linestyle='-', linewidth='0.2', color='grey')\n",
    "    axs[1].grid(linestyle='-', linewidth='0.2', color='grey')\n",
    "    \n",
    "    # Summarize history for accuracy\n",
    "    axs[0].plot(range(1,len(model_details.history['acc'])+1),\n",
    "                model_details.history['acc'])\n",
    "    axs[0].plot(range(1,len(model_details.history['val_acc'])+1),\n",
    "                model_details.history['val_acc'])\n",
    "    axs[0].set_title('Model Accuracy')\n",
    "    axs[0].set_ylabel('Accuracy')\n",
    "    axs[0].set_xlabel('Epoch')\n",
    "    axs[0].set_xticks(np.arange(1,len(model_details.history['acc'])+1),\n",
    "                      len(model_details.history['acc'])/10)\n",
    "    axs[0].legend(['Training Accuracy', 'Validation Accuracy'], loc='best')\n",
    "    \n",
    "    # Summarize history for loss\n",
    "    axs[1].plot(range(1,len(model_details.history['loss'])+1),\n",
    "                model_details.history['loss'])\n",
    "    axs[1].plot(range(1,len(model_details.history['val_loss'])+1),\n",
    "                model_details.history['val_loss'])\n",
    "    axs[1].set_title('Model Loss')\n",
    "    axs[1].set_ylabel('Loss')\n",
    "    axs[1].set_xlabel('Epoch')\n",
    "    axs[1].set_xticks(np.arange(1,len(model_details.history['loss'])+1),\n",
    "                      len(model_details.history['loss'])/10)\n",
    "    axs[1].legend(['Training loss', 'Validation Loss'], loc='best')\n",
    "    \n",
    "    # Show the plot\n",
    "    return plt.show()\n",
    "\n",
    "# Print figure with 10 random images from each\n",
    "def show_imgs(X):\n",
    "    plt.figure(1)\n",
    "    k = 0\n",
    "    for i in range(0,4):\n",
    "        for j in range(0,4):\n",
    "            plt.subplot2grid((4,4),(i,j))\n",
    "            plt.imshow(toimage(X[k]))\n",
    "            k = k+1\n",
    "    # show the plot\n",
    "    return plt.show()\n",
    "\n",
    "def predict_classes(model, images_test, labels_test):\n",
    "    \n",
    "    # Predict class of image using model\n",
    "    class_pred = model.predict(images_test, batch_size=32)\n",
    "\n",
    "    # Convert vector to a label\n",
    "    labels_pred = np.argmax(class_pred,axis=1)\n",
    "    labels_true = np.argmax(labels_test,axis=1)\n",
    "\n",
    "    # Check which labels have been predicted correctly\n",
    "    # Boolean array that tell if predicted label is the true label\n",
    "    correct = (labels_pred == labels_true)\n",
    "    print(\"Number of correct predictions: %d\" % sum(correct))\n",
    "\n",
    "    \n",
    "    # Array which tells if the prediction is correct or not\n",
    "    # And predicted labels\n",
    "    return correct, labels_pred\n",
    "\n",
    "def visualize_errors(images_test, labels_test, class_names, labels_pred, correct):\n",
    "    \n",
    "    incorrect = (correct == False)\n",
    "    \n",
    "    # Images of the test-set that have been incorrectly classified.\n",
    "    images_error = images_test[incorrect]\n",
    "    \n",
    "    # Get predicted classes for those images\n",
    "    labels_error = labels_pred[incorrect]\n",
    "\n",
    "    # Get true classes for those images\n",
    "    labels_true = labels_test[incorrect]\n",
    "    \n",
    "    \n",
    "    # Plot the first 9 images.\n",
    "    return plot_images(images=images_error[0:9],\n",
    "                       labels_true=labels_true[0:9],\n",
    "                       class_names=class_names,\n",
    "                       labels_pred=labels_error[0:9])\n",
    "\n",
    "\n",
    "def plot_confusion_matrix(cm, classes,\n",
    "                          normalize=False,\n",
    "                          title='Confusion matrix',\n",
    "                          cmap=plt.cm.Blues):\n",
    "    \"\"\"\n",
    "    This function prints and plots the confusion matrix.\n",
    "    Normalization can be applied by setting `normalize=True`.\n",
    "    \"\"\"\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(len(classes))\n",
    "    plt.xticks(tick_marks, classes, rotation=45)\n",
    "    plt.yticks(tick_marks, classes)\n",
    "\n",
    "    if normalize:\n",
    "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "\n",
    "    thresh = cm.max() / 2.\n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        plt.text(j, i, cm[i, j],\n",
    "                 horizontalalignment=\"center\",\n",
    "                 color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')\n",
    "\n",
    "    return plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set callback functions to early stop training if a convergence criteria is met (e.g no improvement after 5 epochs)\n",
    "# and save the best model so far \n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=5)\n",
    "\n",
    "# Save the model after every epoch\n",
    "check_point = ModelCheckpoint(filepath='best_model.h5',  # model filename\n",
    "                              monitor='val_loss',        # quantity to monitor\n",
    "                              verbose=0,                 # verbosity - 0 or 1\n",
    "                              save_best_only= True,      # The latest best model will not be overwritten\n",
    "                              mode='auto')               # The decision to overwrite model is made \n",
    "                                                         # automatically depending on the quantity to monitor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load CIFAR-10 dataset\n",
    "\n",
    "Next, we can load the CIFAR-10 data set provided by Keras.\n",
    "\n",
    "The test batch contains exactly 1000 randomly-selected images from each class.\n",
    "\n",
    "The training batches contain the remaining images in random order, but some training batches may contain more images from one class than another.\n",
    "\n",
    "Between them, the training batches contain exactly 5000 images from each class.\n",
    "\n",
    "We can fit this model with 25 epochs and a batch size of 32. A small number of epochs was chosen to help keep this tutorial moving. Normally the number of epochs would be one or two orders of magnitude larger and batch size (e.g. 128) for this problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# number of images used in each optimization step\n",
    "batch_size = 25 \n",
    "\n",
    "# number of one forward and one backward pass of all the training data\n",
    "epochs = 32\n",
    "\n",
    "# one class per digit\n",
    "num_classes = 10\n",
    "\n",
    "# input image dimensions (image resolution)\n",
    "img_rows, img_cols = 32, 32\n",
    "\n",
    "# input shape\n",
    "input_shape = (img_rows, img_cols, 1)\n",
    "\n",
    "# number of convolutional filters to use\n",
    "filters = 32\n",
    "\n",
    "# size of pooling area for max pooling\n",
    "nb_pool = 2\n",
    "\n",
    "# convolution kernel size\n",
    "nb_conv = 3\n",
    "\n",
    "# optimisation parameters\n",
    "learning_rate = 0.0001\n",
    "droprate      = 0.25"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download and process your dataset\n",
    "(x_train, y_train), (x_test, y_test) = cifar10.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here are the classes in the dataset, as well as 10 random images from each\n",
    "class_names = ['airplane','automobile','bird','cat','deer',\n",
    "               'dog','frog','horse','ship','truck']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the first images from the train-set.\n",
    "images_train = x_train[0:9]\n",
    "\n",
    "# Get the true classes for those images.\n",
    "labels_true = y_train[0:9]\n",
    "\n",
    "# Plot the images and labels.\n",
    "<insert code here>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add the depth in the input. For grayscale so depth is only one\n",
    "if K.image_data_format() == 'channels_first':\n",
    "    x_train = x_train.reshape(x_train.shape[0], 3, img_rows, img_cols)\n",
    "    x_test  = x_test.reshape(x_test.shape[0], 3,   img_rows, img_cols)\n",
    "    input_shape = (3, img_rows, img_cols)\n",
    "else:\n",
    "    x_train = x_train.reshape(x_train.shape[0], img_rows, img_cols, 3)\n",
    "    x_test  = x_test.reshape(x_test.shape[0],   img_rows, img_cols, 3)\n",
    "    input_shape = (img_rows, img_cols, 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The pixel values are in the range of 0 to 255 for each of the red, green and blue channels.\n",
    "\n",
    "It is good practice to work with normalized data. Because the input values are well understood, we can easily normalize to the range 0 to 1 by dividing each value by the maximum observation which is 255.\n",
    "\n",
    "Note, the data is loaded as integers, so we must cast it to floating point values in order to perform the division."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change to float datatype\n",
    "x_train = x_train.astype('float32')\n",
    "x_test  = x_test.astype('float32')\n",
    "\n",
    "# Normalize inputs from 0-255 to 0-1\n",
    "<insert code here for train>\n",
    "<insert code here for test>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the shapes to check if everything's ok\n",
    "print(\"Number of training samples: %d\" % x_train.shape[0])\n",
    "print(\"Number of test samples: %d\" % x_test.shape[0])\n",
    "print(\"Image rows: %d\" % x_train.shape[1])\n",
    "print(\"Image columns: %d\" % x_train.shape[2])\n",
    "print(\"Number of classes: %d\" % num_classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use a one hot encoding to transform them into a binary matrix in order to best model the classification problem. We know there are 10 classes for this problem, so we can expect the binary matrix to have a width of 10."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert class vectors to binary class matrices (ie one-hot encoded vectors)\n",
    "y_train = keras.utils.to_categorical(y_train, num_classes)\n",
    "y_test  = keras.utils.to_categorical(y_test, num_classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Additional information\n",
    "\n",
    "### Max Pooling Layer\n",
    "\n",
    "Pooling layer is mostly used immediately after the convolutional layer to reduce the spatial size (only width and height, not depth). This reduces the number of parameters, hence computation is reduced. Using fewer parameters avoids overfitting.\n",
    "\n",
    "Note: Overfitting is the condition when a trained model works very well on training data, but does not work very well on test data.\n",
    "\n",
    "### Regularization\n",
    "\n",
    "Deep neural nets with a large number of parameters are very powerful machine learning systems. However, overfitting is a serious problem in such networks.  \n",
    "\n",
    "- **Dropout:** the key idea is to randomly drop units (along with their connections) from the neural network during training. The reduction in number of parameters in each step of training has effect of regularization. Dropout has shown improvements in the performance of neural networks on supervised learning tasks in vision, speech recognition, document classification and computational biology, obtaining state-of-the-art results on many benchmark data sets.\n",
    "\n",
    "- **Kernel_initializer:** it allows to apply penalties on layer parameters during optimization. These penalties are incorporated in the loss function that the network optimizes. This argument in convolutional layer can be for example L2 regularisation of the weights (or random_uniform, glorot_normal, he_normal). This penalizes spiky weights and makes sure that all the inputs are considered. During gradient descent parameter update, the above L2 regularization ultimately means that every weight is decayed linearly, that’s why called weight decay.\n",
    "\n",
    "- **BatchNormalization:**  normalizes the activation of the previous layer at each batch, i.e. applies a transformation that maintains the mean activation close to 0 and the activation standard deviation close to 1. It addresses the problem of internal covariate shift. It also acts as a regularizer, in some cases eliminating the need for Dropout. Batch Normalization achieves the same accuracy with fewer training steps thus speeding up the training process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For example to constructor\n",
    "def create_model(input_shape=(32, 32, 3)):\n",
    "    # Start neural network\n",
    "    <insert code here>\n",
    "  \n",
    "    # Add input_shape convolutional layer with 32 filters and 3x3 kernel size, a ReLU activation function (use padding \"same\")\n",
    "    <insert code here>\n",
    "    \n",
    "    # Add convolutional layer with 32 filters and 3x3 kernel size, and a ReLU activation function\n",
    "    <insert code here>\n",
    "    # Add pooling layer (e.g. max pooling) with 2x2 patch size\n",
    "    <insert code here>\n",
    "    # Add dropout layer (e.g 25%)\n",
    "    <insert code here>\n",
    " \n",
    "    # Add convolutional layer with 64 filters and 3x3 kernel size, and a ReLU activation function\n",
    "    <insert code here>\n",
    "    # Add convolutional layer with 64 filters and 3x3 kernel size, and a ReLU activation function\n",
    "    <insert code here>\n",
    "    # Add pooling layer (e.g. max pooling) with 2x2 patch\n",
    "    <insert code here>\n",
    "    # Add dropout layer (e.g 25%)\n",
    "    <insert code here>\n",
    " \n",
    "    # Add convolutional layer with 64 filters and 3x3 kernel size, and a ReLU activation function\n",
    "    <insert code here>\n",
    "    # Add convolutional layer with 64 filters and 3x3 kernel size, and a ReLU activation function\n",
    "    <insert code here>\n",
    "    # Add pooling layer (e.g. max pooling) with 2x2 patch size\n",
    "    <insert code here>\n",
    "    # Add dropout layer (e.g 25%)\n",
    "    <insert code here>\n",
    " \n",
    "    # Add fully connected layer \n",
    "    <insert code here>\n",
    "    # Add dense layer 512 unites with a ReLU activation function\n",
    "    <insert code here>\n",
    "    # Add dropout layer (e.g. 50%)\n",
    "    <insert code here>\n",
    "    \n",
    "    # Add fully connected output layer with 10 units and a softmax activation function\n",
    "    <insert code here>\n",
    "   \n",
    "    # Print model summary\n",
    "    <insert code here>)\n",
    "    \n",
    "    # Compile neural network, use Root Mean Square Propagation (rmsprop) for optimization, \n",
    "    # categorical_crossentropy for loss,\n",
    "    # and accuracy (aka acc) for performance metrics\n",
    "    <insert code here>\n",
    "    \n",
    "    # return model\n",
    "    <insert code here>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instatiate model object\n",
    "<insert code here>\n",
    "\n",
    "# Training model (include batch size, and number of epocs, ) \n",
    "# set validation_split to 20%, and store object in a variable named model_history\n",
    "<insert code here>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the model store evaluation in a variable named scores\n",
    "<insert code here>\n",
    "\n",
    "print('Test result: %.3f loss: %.3f' % (scores[1]*100, scores[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The classification accuracy and loss is printed each epoch on both the training and test datasets. The model is evaluated on the test set and achieves an accuracy hopefully above 50%.\n",
    "\n",
    "We can improve the accuracy significantly by creating a much deeper network. This is what we will look at in the next section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model accuracy and loss plots (feed function model_history from above)\n",
    "<Insert code here>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Confusion matrix** – also known as an error matrix, is a specific table layout that allows visualization of the performance of an algorithm, typically a supervised learning one (in unsupervised learning it is usually called a matching matrix). Each row of the matrix represents the instances in a predicted class while each column represents the instances in an actual class (or vice versa)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict class of image using model for test dataset, use batch size 32, \n",
    "# and store output in a variable named y_pred\n",
    "<insert code here>\n",
    "\n",
    "# Convert vector to a label\n",
    "labels_true = np.argmax(y_test,axis=1)\n",
    "labels_pred = np.argmax(y_pred,axis=1)\n",
    "\n",
    "# Calculate confusion matrix result store in variabled called cm\n",
    "<insert code here>\n",
    "\n",
    "# Plot confusion matrix (can use helper function), \n",
    "# try changing the normalization True and False\n",
    "<insert code here>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use predict_classes helper function to obtain t\n",
    "# he predicted label index for correct ones\n",
    "<Insert code here>\n",
    "\n",
    "# Plot the first 9 mis-classified images\n",
    "<Insert code here>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Improve Model Performance\n",
    "\n",
    "We have achieved good results on this very complex problem, but we are still a good way from achieving world class results.  In this section we look at scaling up the size and complexity of our model.\n",
    "\n",
    "Below are some ideas that you can try to extend upon the models and improve model performance.\n",
    "\n",
    "- **Train for More Epochs**. Each model was trained for a very small number of epochs, 25. It is common to train large convolutional neural networks for hundreds or thousands of epochs. I would expect that performance gains can be achieved by significantly raising the number of training epochs.\n",
    "\n",
    "- **Deeper Network Topology**. The larger network presented is deep, but larger networks could be designed for the problem. This may involve more feature maps closer to the input and perhaps less aggressive pooling. Additionally, standard convolutional network topologies that have been shown useful may be adopted and evaluated on the problem. Below we will try deeper networks and also try a couple of state-of-the-art image recognition architectures, e.g. VGG-16 and  AlexNet.\n",
    "\n",
    "- **Image Data Augmentation**. The objects in the image vary in their position. Another boost in model performance can likely be achieved by using some data augmentation. Methods such as standardization and random shifts and horizontal image flips may be beneficial."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build fist variant 4-layer model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4-layer cnn model\n",
    "def base_4_layer_model(input_shape=(32, 32, 3)):\n",
    " \n",
    "    # 0. Start Sequential model\n",
    "    <insert code here>\n",
    "  \n",
    "    # 1. Convolutional input layer, 32 feature maps with a size of 3×3, a ReLU activation function\n",
    "    <insert code here>\n",
    "    <insert code here>\n",
    "    # 2. Convolutional input layer, 32 feature maps with a size of 3×3, a ReLU activation function\n",
    "    <insert code here>\n",
    "    <insert code here>\n",
    "    # 3. Max Pool layer with size 2×2\n",
    "    <insert code here>\n",
    "    # 4. Dropout set to 25%\n",
    "    <insert code here>\n",
    "    # 5. Convolutional input layer, 64 feature maps with a size of 3×3, a ReLU activation function\n",
    "    <insert code here>\n",
    "    <insert code here>\n",
    "    # 6. Convolutional input layer, 64 feature maps with a size of 3×3, a ReLU activation function\n",
    "    <insert code here>\n",
    "    <insert code here>\n",
    "    # 7. Max Pool layer with size 2×2\n",
    "    <insert code here>\n",
    "    # 8. Dropout set to 25%\n",
    "    <insert code here>\n",
    " \n",
    "    # 9. Flatten layer\n",
    "    <insert code here>\n",
    "    \n",
    "    # 10. Fully connected layer with 512 units and a ReLU activation function\n",
    "    <insert code here>\n",
    "    # 11. Dropout set to 50%\n",
    "    <insert code here>\n",
    "    # 12. Fully connected output layer with 10 units and a softmax activation function\n",
    "    <insert code here>\n",
    "    \n",
    "    # 13. A logarithmic loss function is used with the stochastic gradient \n",
    "    # descent optimization algorithm configured with a large momentum and \n",
    "    # weight decay start with a learning rate of 0.1.\n",
    "    sgd = SGD(lr=0.01, decay=1e-6, momentum=0.9, nesterov=True)\n",
    " \n",
    "    model.summary()\n",
    " \n",
    "    model.compile(loss='categorical_crossentropy', \n",
    "                  optimizer=sgd, \n",
    "                  metrics=['accuracy'])\n",
    "  \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate 4-layer model\n",
    "<insert code here>\n",
    "\n",
    "# Fit model (include batch size, epochs, validation split set to 20%, shuffle True)\n",
    "# Store model training in variable called history_cnn_4_layer_model\n",
    "<insert code here>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the model store evaluation in a variable named scores\n",
    "<insert code here>\n",
    "\n",
    "print('Test result: %.3f loss: %.3f' % (scores[1]*100,scores[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model accuracy and loss plots (feed function model_history from above)\n",
    "<Insert code here>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build second variant of 6-layer model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6-layer cnn model\n",
    "def base_6_layer_model(input_shape=(32, 32, 3)):\n",
    "  \n",
    "    # 0. Start Sequetial model\n",
    "    <insert code here>\n",
    "    # 1. Convolutional input layer, 32 feature maps with a size of 3×3, a rectifier activation function\n",
    "    <insert code here>\n",
    "    # 2. Convolutional input layer, 32 feature maps with a size of 3×3, a rectifier activation function\n",
    "    <insert code here>\n",
    "    # 3. Max Pool layer with size 2×2\n",
    "    <insert code here>\n",
    "    # 4. Dropout set to 25%\n",
    "    <insert code here>\n",
    "\n",
    "    # 5. Convolutional input layer, 64 feature maps with a size of 3×3, a rectifier activation function\n",
    "    <insert code here>\n",
    "    <insert code here>\n",
    "    # 6. Convolutional input layer, 64 feature maps with a size of 3×3, a rectifier activation function\n",
    "    <insert code here>\n",
    "    <insert code here>\n",
    "    # 7. Max Pool layer with size 2×2\n",
    "    <insert code here>\n",
    "    # 8. Dropout set to 25%\n",
    "    <insert code here>\n",
    "\n",
    "    # 9. Flatten layer\n",
    "    <insert code here>\n",
    "    # 10. Fully connected layer with 512 units and a rectifier activation function\n",
    "    <insert code here>\n",
    "    <insert code here>\n",
    "    # 11. Dropout set to 50%\n",
    "    <insert code here>\n",
    "    # 12. Fully connected output layer with 10 units and a softmax activation function\n",
    "    <insert code here>\n",
    "    <insert code here>\n",
    "\n",
    "    # 13. Model summary\n",
    "    model.summary()\n",
    "    \n",
    "    # 14. A logarithmic loss function is used with the stochastic gradient descent \n",
    "    # optimization algorithm configured with a large momentum and weight decay \n",
    "    # start with a learning rate of 0.1.\n",
    "    sgd = SGD(lr = 0.1, decay=1e-6, momentum=0.9 nesterov=True)\n",
    "\n",
    "    # 15. Compile model use categorical cross entropy and accuracy for validation metric\n",
    "    model.compile(loss='categorical_crossentropy', optimizer=sgd, metrics=['accuracy'])\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate 6-layer model\n",
    "<insert code here>\n",
    "\n",
    "# Fit model (include batch size, epochs, validation split set to 20%, shuffle True)\n",
    "# Store model training in variable called history_cnn_4_layer_model\n",
    "<insert code here>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the model store evaluation in a variable named scores\n",
    "<insert code here>\n",
    "\n",
    "print('Test result: %.3f loss: %.3f' % (scores[1]*100,scores[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model accuracy and loss plots (feed function model_history from above)\n",
    "<Insert code here>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Try 2 state-of-the-art CNN architectures\n",
    "\n",
    "###  VGG-16 architectures\n",
    "\n",
    "VGG16 was publised in 2014 and is one of the simplest (among the other cnn architectures used in Imagenet competition). It's Key Characteristics are:\n",
    "\n",
    "\n",
    "1.   This network contains total 16 layers in which weights and bias parameters are learnt.\n",
    "2.   A total of 13 convolutional layers are stacked one after the other and 3 dense layers for classification.\n",
    "3.   The number of filters in the convolution layers follow an increasing pattern (similar to decoder architecture of autoencoder).\n",
    "4.   The informative features are obtained by max pooling layers applied at different steps in the architecture.\n",
    "5.   The dense layers comprises of 4096, 4096, and 1000 nodes each (can use 10 instead).\n",
    "6.   The cons of this architecture are that it is slow to train and produces the model with very large size.\n",
    "\n",
    "![Image of VGG-16](https://qph.fs.quoracdn.net/main-qimg-83c7dee9e8b039c3ca27c8dd91cacbb4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# VGG-16 model using Keras' functional API method\n",
    "def build_vgg16_model(input_shape=(32, 32, 3)):\n",
    "    # input layer\n",
    "    inputs = Input(shape=input_shape, name='input')\n",
    "  \n",
    "    conv1  = Conv2D(filters=64, kernel_size=(3,3), padding=\"same\", activation=\"relu\")(inputs)\n",
    "    conv2  = Conv2D(filters=64, kernel_size=(3,3), padding=\"same\", activation=\"relu\")(conv1)\n",
    "    pool1  = MaxPooling2D((2, 2))(conv2)\n",
    "\n",
    "    conv3  = Conv2D(filters=128, kernel_size=(3,3), padding=\"same\", activation=\"relu\")(pool1)\n",
    "    conv4  = Conv2D(filters=128, kernel_size=(3,3), padding=\"same\", activation=\"relu\")(conv3)\n",
    "    pool2  = MaxPooling2D((2, 2))(conv4)\n",
    "\n",
    "    conv5  = Conv2D(filters=256, kernel_size=(3,3), padding=\"same\", activation=\"relu\")(pool2)\n",
    "    conv6  = Conv2D(filters=256, kernel_size=(3,3), padding=\"same\", activation=\"relu\")(conv5)\n",
    "    conv7  = Conv2D(filters=256, kernel_size=(3,3), padding=\"same\", activation=\"relu\")(conv6)\n",
    "    pool3  = MaxPooling2D((2, 2))(conv7)\n",
    "\n",
    "    conv8  = Conv2D(filters=512, kernel_size=(3,3), padding=\"same\", activation=\"relu\")(pool3)\n",
    "    conv9  = Conv2D(filters=512, kernel_size=(3,3), padding=\"same\", activation=\"relu\")(conv8)\n",
    "    conv10 = Conv2D(filters=512, kernel_size=(3,3), padding=\"same\", activation=\"relu\")(conv9)\n",
    "    pool4  = MaxPooling2D((2, 2))(conv10)\n",
    "\n",
    "    conv11 = Conv2D(filters=512, kernel_size=(3,3), padding=\"same\", activation=\"relu\")(pool4)\n",
    "    conv12 = Conv2D(filters=512, kernel_size=(3,3), padding=\"same\", activation=\"relu\")(conv11)\n",
    "    conv13 = Conv2D(filters=512, kernel_size=(3,3), padding=\"same\", activation=\"relu\")(conv12)\n",
    "    pool5  = MaxPooling2D((2, 2))(conv13)\n",
    "\n",
    "    flat   = Flatten()(pool5)\n",
    "    dense1 = Dense(4096, activation=\"relu\")(flat)\n",
    "    dense2 = Dense(4096, activation=\"relu\")(dense1)\n",
    "    output = Dense(10, activation=\"softmax\")(dense2)\n",
    "  \n",
    "    model  = Model(inputs=inputs, outputs=output)\n",
    "    \n",
    "    model.summary()\n",
    "    \n",
    "    # optimizer\n",
    "    sgd = SGD(lr=0.1, decay=1e-6, momentum=0.9, nesterov=True)\n",
    "  \n",
    "    model.compile(loss='categorical_crossentropy', \n",
    "                  optimizer=sgd, \n",
    "                  metrics=['accuracy'])\n",
    "  \n",
    "  return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vgg16_model = build_VGG_16_model()\n",
    "history_vgg16_model.fit(x_train, y_train,\n",
    "                        batch_size=batch_size,\n",
    "                        epochs=epochs,\n",
    "                        validation_split=0.20,\n",
    "                        shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the model store evaluation in a variable named scores\n",
    "<insert code here>\n",
    "\n",
    "print('Test result: %.3f loss: %.3f' % (scores[1]*100,scores[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model accuracy and loss plots (feed function model_history from above)\n",
    "<Insert code here>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### AlexNet architectures\n",
    "\n",
    "contains 8 layers of transformations, five convolutional layers followed by two fully connected hidden layers and an output layer.\n",
    "\n",
    "The convolutional kernels in the first convolutional layer are reasonably large at 11×1111×11, in the second they are 5×55×5 and thereafter they are 3×33×3. Moreover, the first, second, and fifth convolutional layers are each followed by overlapping pooling operations with pool size 3×33×3 and stride (2×22×2).\n",
    "\n",
    "Following the convolutional layers, the original AlexNet had fully-connected layers with 4096 nodes each.\n",
    "\n",
    "![Image of AlexNet](https://www.oreilly.com/library/view/tensorflow-for-deep/9781491980446/assets/tfdl_0106.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_alexnet_model(input_shape=(32, 32, 3)):\n",
    "    model = Sequential()\n",
    "    # First convolutional layer\n",
    "    model.add(Conv2D(filters=96, kernel_size=(11,11), strides=(4,4),\n",
    "                     activation='relu',\n",
    "                     input_shape=input_shape))\n",
    "    model.add(MaxPool2D(pool_size=3, strides=2))\n",
    "\n",
    "    # Second convolutional layer\n",
    "    model.add(Conv2D(filters=192, kernel_size=(5,5), activation='relu'))\n",
    "    model.add(MaxPool2D(pool_size=3, strides=(2,2)))\n",
    "\n",
    "    # Third convolutional layer\n",
    "    model.add(Conv2D(filters=384, kernel_size=(3,3), activation='relu'))\n",
    "\n",
    "    # Fourth convolutional layer\n",
    "    model.add(Conv2D(filters=384, kernel_size=(3,3), activation='relu'))\n",
    "\n",
    "    # Fifth convolutional layer\n",
    "    model.add(Conv2D(filters=256, kernel_size=(3,3), activation='relu'))\n",
    "    model.add(MaxPool2D(pool_size==(3,3), strides=2))\n",
    "\n",
    "    # Flatten and apply fullly connected layers\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(4096, activation=\"relu\"))\n",
    "    model.add(Dense(4096, activation=\"relu\"))\n",
    "  \n",
    "    model.summary()\n",
    "  \n",
    "    model.compile(loss=\"categorical_crossentropy\", \n",
    "                  optimizer='adadelta', \n",
    "                  metrics=[\"accuracy\"])\n",
    "  \n",
    "  return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate AlexNet model\n",
    "<inster code here>\n",
    "\n",
    "# Traing AlexNet model on training data, include batch size, epochs, validation split\n",
    "# and remember to shuffle, store result in variable called history_alexnet_model\n",
    "<insert code>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the model store evaluation in a variable named scores\n",
    "<insert code here>\n",
    "\n",
    "print('Test result: %.3f loss: %.3f' % (scores[1]*100,scores[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model accuracy and loss plots (feed function model_history from above)\n",
    "<Insert code here>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "VGG16 makes the improvement over AlexNet by replacing large kernel-sized filters(11 and 5 in the first and second convolutional layer, respectively) with multiple 3X3 kernel-sized filters one after another. Having multiple stacked smaller size kernel is better than the one with a larger size kernel because multiple non-linear layers increases the depth of the network which enables it to learn more complex features, and that too at a lower cost. \n",
    "\n",
    "We can say from the above that the initial layers are looking at smaller regions of the image and thus can only learn simple features like edges / corners etc. As we go deeper into the network, the neurons get information from larger parts of the image and from various other neurons. Thus, the neurons at the later layers can learn more complicated features e.g. wheels, windows, etc.\n",
    "\n",
    "From the above curves, we can see that there is a considerable difference between the training and validation loss. This indicates that the network has tried to memorize the training data and thus, is able to get better accuracy on it. This is a sign of Overfitting. But we have already used Dropout in the network, then why is it still overfitting. Let us see if we can further reduce overfitting using something else.\n",
    "\n",
    "## Data Augmentation\n",
    "\n",
    "In Keras, We have a ImageDataGenerator class that is used to generate batches of tensor image data with real-time data augmentation. The data will be looped over (in batches) indefinitely. The image data is generated by transforming the actual training images by rotation, crop, shifts, shear, zoom, flip, reflection, normalization etc. The below code snippets shows how to initialize the image data generator class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First step pick the CNN architecture selected for model improvement\n",
    "def cnn_model(input_shape=(32,32,3)):\n",
    "    \n",
    "    model = Sequential()\n",
    "    \n",
    "    model.add(Conv2D(filters=96, kernel_size=(3, 3), activation='relu', padding = 'same', input_shape=input_shape))    \n",
    "    model.add(Dropout(0.2))\n",
    "    \n",
    "    model.add(Conv2D(filters=96, kernel_size=(3, 3), activation='relu', padding = 'same'))  \n",
    "    model.add(Conv2D(filters=96, kernel_size=(3, 3), activation='relu', padding = 'same', strides = 2))    \n",
    "    model.add(Dropout(0.5))\n",
    "    \n",
    "    model.add(Conv2D(filters=192, kernel_size=(3, 3), activation='relu', padding = 'same'))    \n",
    "    model.add(Conv2D(filters=192, kernel_size=(3, 3), activation='relu', padding = 'same'))\n",
    "    model.add(Conv2D(filters=192, kernel_size=(3, 3), activation='relu', padding = 'same', strides = 2))    \n",
    "    model.add(Dropout(0.5))    \n",
    "    \n",
    "    model.add(Conv2D(filters=192, kernel_size=(3, 3), padding = 'same'))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(Conv2D(filters=192, kernel_size=(1, 1),padding='valid'))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(Conv2D(filters=10, kernel_size=(1, 1), padding='valid'))\n",
    "\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "    \n",
    "    \n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(num_classes, activation='softmax'))\n",
    "\n",
    "    model.summary()\n",
    "    \n",
    "    # Configure the model for training\n",
    "    model.compile(loss='categorical_crossentropy', # Better loss function for neural networks\n",
    "                  optimizer=Adam(lr=1.0e-4), # Adam optimizer with 1.0e-4 learning rate\n",
    "                  metrics = ['accuracy'])   # Metrics to be evaluated by the model\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instatiate model\n",
    "model = cnn_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a data generature function ready for fitting on data\n",
    "def create_datagen(x, y, rescale=None, batch_size=64):\n",
    "    # Data augmentation to have random shifts, rotations and flips, thus increasing the size of the dataset.\n",
    "    image_generator = ImageDataGenerator(\n",
    "        featurewise_center=False,  # set input mean to 0 over the dataset\n",
    "        samplewise_center=False,   # set each sample mean to 0\n",
    "        featurewise_std_normalization=False,  # divide inputs by std of the dataset\n",
    "        samplewise_std_normalization=False,   # divide each input by its std\n",
    "        zca_whitening=False,     # apply ZCA whitening\n",
    "        rotation_range=45,       # randomly rotate images in the range (degrees, 0 to 180)\n",
    "        width_shift_range=0.2,   # randomly shift images horizontally (fraction of total width)\n",
    "        height_shift_range=0.2,  # randomly shift images vertically (fraction of total height)\n",
    "        horizontal_flip=True,    # randomly flip images horizontally\n",
    "        vertical_flip=False)     # randomly flip images vertically\n",
    "  \n",
    "    # Generate augmented data\n",
    "    data_generator = image_generator.flow(x, y, batch_size=batch_size)\n",
    "  \n",
    "    return data_generator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualizing the images generated using flow method of ImageDataGenerator class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure batch size and retrieve one batch of images\n",
    "for X_batch, y_batch in create_datagen(x_train, y_train, batch_size=9):\n",
    "    # Show 9 images\n",
    "    for i in range(0, 9):\n",
    "        plt.subplot(330 + 1 + i)\n",
    "        plt.imshow(toimage(X_batch[i].reshape(img_rows, img_cols, 3)))\n",
    "    # show the plot\n",
    "    plt.show()\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\bf{Note:}$ Flow generates batches of data, after performing the data transformations and augmentation specified during the instantiation of the data generator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a training data generator object (use helper function create_datagen), use batch size 64\n",
    "# store result in a variable called train_generator\n",
    "<insert code here>\n",
    "\n",
    "# Create a training data generator object (use helper function create_datagen), use batch size 65\n",
    "# this is the augmentation configuration we will use for testing (only rescaling, 1./255)\n",
    "# store result in a variable called test_generator\n",
    "<insert code here>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit the model on the data provided\n",
    "model.fit_generator(train_generator,\n",
    "                    steps_per_epoch=x_train.shape[0] // batch_size,  # number of samples per gradient update\n",
    "                    epochs=5, # number of iterations\n",
    "                    validation_data=test_generator, #(x_test,y_test)\n",
    "                    verbose=1, #10\n",
    "                    callbacks=[check_point, early_stopping] # we make use of check point, and early stopy to prevent overfitting\n",
    "                   )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the model store evaluation in a variable named scores\n",
    "<insert code here>\n",
    "\n",
    "print('Test result: %.3f loss: %.3f' % (scores[1]*100,scores[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model accuracy and loss plots (feed function model_history from above)\n",
    "<Insert code here>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How to construct Scikit-learn classifier from Keras model\n",
    "\n",
    "Let’s get started.\n",
    "\n",
    "In this section, I want to show you how to wrap Keras models for use in scikit-learn and how to use grid search hyper-parameters, such as learning rate, dropout rate, epochs and number of neurons.\n",
    "\n",
    "\n",
    "To use these wrappers you must define a function that creates and returns your Keras sequential model, then pass this function to the build_fn argument when constructing the KerasClassifier class.\n",
    "\n",
    "\n",
    "The constructor for the KerasClassifier class can take default arguments that are passed on to the calls to model.fit(), such as the number of epochs and the batch size. But can also take new argument, e.g. number of layers, to be passed to your custom create_model() function with default values.\n",
    "\n",
    "\n",
    "For example:\n",
    "\n",
    "> model = KerasClassifier(build_fn=create_model,  epochs=10, dropout_rate=0.2)\n",
    "\n",
    "\n",
    "### How to Use Grid Search in scikit-learn\n",
    "\n",
    "Grid search is a model hyperparameter optimization technique. In scikit-learn this technique is provided in the GridSearchCV class.\n",
    "\n",
    "When constructing this class you must provide a dictionary of hyperparameters to evaluate in the param_grid argument. This is a map of the model parameter name and an array of values to try.\n",
    "\n",
    "By default, accuracy is the score that is optimized, but other scores can be specified in the score argument of the GridSearchCV constructor.\n",
    "\n",
    "The GridSearchCV process will then construct and evaluate one model for each combination of parameters. Cross validation is used to evaluate each individual model and the default of 3-fold cross validation is used, although this can be overridden by specifying the cv argument to the GridSearchCV constructor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define custom build CNN architecture (example)\n",
    "def build_fn(input_shape=(32, 32, 3), kernel_initializer='he_uniform', optimizers='adam'):\n",
    "    model = Sequential()\n",
    "    model.add(Conv2D(filters=32, kernel_size=(3,3), padding='same', \n",
    "                     kernel_initializer=kernel_initializer, \n",
    "                     input_shape=input_shape))\n",
    "    model.add(Activation('elu'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Conv2D(filters=32, kernel_size=(3,3), padding='same', \n",
    "                     kernel_initializer=kernel_initializer))\n",
    "    model.add(Activation('elu'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "    model.add(Dropout(0.2))\n",
    " \n",
    "    model.add(Conv2D(filters=64, kernel_size=(3,3), padding='same', \n",
    "                     kernel_initializer=kernel_initializer))\n",
    "    model.add(Activation('elu'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Conv2D(filters=64, kernel_size=(3,3), padding='same', \n",
    "                     kernel_initializer=kernel_initializer))\n",
    "    model.add(Activation('elu'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "    model.add(Dropout(0.3))\n",
    " \n",
    "    model.add(Conv2D(filters=128, kernel_size=(3,3), padding='same', \n",
    "                     kernel_initializer=kernel_initializer))\n",
    "    model.add(Activation('elu'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Conv2D(filters=128, kernel_size=(3,3), padding='same', \n",
    "                     kernel_initializer=kernel_initializer))\n",
    "    model.add(Activation('elu'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "    model.add(Dropout(0.4))\n",
    " \n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(num_classes, activation='softmax'))\n",
    " \n",
    "    model.summary()\n",
    "  \n",
    "    model.compile(loss='categorical_crossentropy', \n",
    "                  optimizer=optimizers, \n",
    "                  metrics=['acc'])\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Scikit-learn classifier for Keras model\n",
    "estimator = KerasClassifier(build_fn=build_fn, \n",
    "                            epochs=10,\n",
    "                            batch_size=128,\n",
    "                            kernel_initializer='he_normal')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define grid search epochs, batches, kernel initializer, dropout_rate, and optimizers\n",
    "optimizers = ['fill', 'fill']\n",
    "kernel_initializer = ['fill', 'fill']\n",
    "epochs = [1, 1, 1]\n",
    "batches = [1, 1, 1]>\n",
    "dropout_rate = [1, 1, 1]\n",
    "\n",
    "# Create python dictionary with hyer-parameter variable names with corresponding list of values to explore\n",
    "param_grid = dict(optimizer=optimizers, \n",
    "                  epochs=epochs, \n",
    "                  batch_size=batches, \n",
    "                  kernel_initializer=kernel_initializer)\n",
    "\n",
    "# Instantiate GridSearchCV object (handles Cross-Validation)\n",
    "grid_search = GridSearchCV(estimator=estimator, \n",
    "                           param_grid=param_grid,\n",
    "                           n_jobs=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit model on training data store result in variable called grid_result\n",
    "<insert code here>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate The Model with k-Fold Cross Validation\n",
    "\n",
    "The scikit-learn has excellent capability to evaluate models using a suite of techniques. The gold standard for evaluating machine learning models is k-fold cross validation.\n",
    "\n",
    "First we can define the model evaluation procedure. Here, we set the number of folds to be 5 (10 is usually an excellent default) and to shuffle the data before partitioning it.\n",
    "\n",
    "Evaluating the model will take a long while and returns an object that describes the evaluation of the 5 constructed models for each of the splits of the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate using 15-fold cross validation\n",
    "kfold = StratifiedKFold(n_splits=5, shuffle=True, random_state=seed)\n",
    "\n",
    "# Alternative k-fold cross validated type (uncomment to use)\n",
    "# kfold = KFold(n_splits=5, shuffle=True, random_state=seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model performance\n",
    "results = cross_val_score(estimator, x_train, y_ttrain, cv=kfold)\n",
    "print(\"Baseline: %.2f%% (%.2f%%)\" % (results.mean()*100, results.std()*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Produce classification report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_true = y_test\n",
    "y_pred = np.argmax(grid_result.fit(x_test, y_test),axis=1)\n",
    "\n",
    "classification_report(y_true, y_pred, target_names=class_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "In this tutorial you discovered how to create state-of-the-art  deep learning models in Keras for object recognition in photographs.\n",
    "\n",
    "After working through this tutorial you learned:\n",
    "\n",
    "- What is Keras library and how to use it.\n",
    "-  How to use the CIFAR-10 dataset and how to load it in Keras and plot ad hoc examples from the dataset.\n",
    "- What is Convolutional Neural Networks (CNN)\n",
    "- How to build step by step CNN\n",
    "- How to train and evaluate CNN on the problem.\n",
    "- How to expand a simple CNN into a deep architector in order to boost performance on the difficult problem.\n",
    "- What are differences in model results and how to visualize it using confusion matrix, loss and accuracy metric plots\n",
    "- How to use data augmentation to get a further boost on the difficult object recognition problem.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
