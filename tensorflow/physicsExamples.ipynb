{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Physics example 1: classification of SUSY signal over SM background events\n",
    "\n",
    "In the first example we look at building a multi-layered dense neural network with tensorflow to classify SUSY signal events over background events.\n",
    "\n",
    "Further down, in 'Physics example 2', we look at the example of a simple regression test. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import all the required libraries here\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from sklearn import model_selection,preprocessing\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import roc_curve, auc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### First load the dataset and define the variables to train over\n",
    "\n",
    "We will use the popular python analysis library pandas to load and manipulate the data within a dataframe.\n",
    "\n",
    "The dataset we will use has 50K events, 25K of each signal and background. This is indicated by the column 'signal' (0 for background, 1 for signal). Several high and low level physics variables have been precalculated and stored, we will use a typical subset used in SUSY analyses as features for the network to train on.\n",
    "\n",
    "The background consists of ttbar events and the signal consists of the T2tt simplified SUSY model, where stops are pair produced and decay to a weakly interacting LSP and a top quark. For this particular point the mass of the stop is 900 GeV and the mass of the LSP is 100 GeV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfFull = pd.read_pickle('data/stop50K.pkl')\n",
    "\n",
    "#Pick a subset of features from the dataframe\n",
    "subset=['signal', #1 for signal and 0 for background\n",
    "        'HT','MET', #energy sums\n",
    "        'MT','MT2W', #topological variables\n",
    "        'n_jet','n_bjet', #jet and b-tag multiplicities\n",
    "        'sel_lep_pt0','sel_lep_eta0','sel_lep_phi0', #lepton 4-vector\n",
    "        'selJet_phi0','selJet_pt0','selJet_eta0','selJet_m0',# lead jet 4-vector\n",
    "        'selJet_phi1','selJet_pt1','selJet_eta1','selJet_m1',# second jet 4-vector\n",
    "        'selJet_phi2','selJet_pt2','selJet_eta2','selJet_m2']# third jet 4-vector\n",
    "\n",
    "df = dfFull[subset]\n",
    "\n",
    "print 'The df has',(df.signal==1).sum(),'signal events and',(df.signal==0).sum(),'background events'\n",
    "print '\\nSummary statistics:'\n",
    "df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preparing the data for a neural network training: splitting and standardising\n",
    "\n",
    "To prepare the data for machine learning with a neural network we need to split into a testing and training set, to avoid bias when evaluating the performance. We additionally adopt the naming convention of 'y' for target variables and 'X' for input variables.\n",
    "\n",
    "It is also sensible to 'standardise' the input variables for a neural network by scaling them to have a standard deviation between -1 and 1 and a mean of 0. This means all the features have the same effective weight when training the network. \n",
    "\n",
    "For these task we use functions from the useful python ML library 'scikit-learn'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X=df.drop('signal',axis=1)\n",
    "y=df['signal']#.values.reshape(len(df),1)\n",
    "\n",
    "#Split with 30% of data kept aside for testing\n",
    "X_train,X_test,y_train,y_test = model_selection.train_test_split(X, y, test_size=0.3, \n",
    "                                                                 random_state=42)\n",
    "\n",
    "#Now do the standardisation on the train set\n",
    "scaler = preprocessing.StandardScaler().fit(X_train)\n",
    "X_train = pd.DataFrame(scaler.transform(X_train),columns=X_train.columns,index=X_train.index)\n",
    "\n",
    "#Additionally apply the scaling it to the test set\n",
    "X_test = pd.DataFrame(scaler.transform(X_test),columns=X_test.columns,index=X_test.index)\n",
    "\n",
    "#Now check it worked\n",
    "print 'Standardised statistics:'\n",
    "X_train.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building the neural network model\n",
    "\n",
    "Now we have a prepared dataset ready for our neural network we need to build the model with tensorflow. Working from the example from 'gaussian.ipynb', build a 2 hidden layer network with 50 nodes on each layer. Therefore requiring 3 sets of weight and biases. We will choose the activation function of the internal nodes to be a RELU.\n",
    "\n",
    "The input size will be the number of features available in X and the output will be one node with a sigmoid function to choose between 1 or 0 (signal or background)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model(x,input_size,layer_size=50,sigmoid_out=True,scope_name='model'):\n",
    "    #This ensuring the variables are defined within the scope of the model\n",
    "    \n",
    "    ###### YOUR CODE HERE ######\n",
    "\n",
    "    return logits, tf.sigmoid(logits)\n",
    "    \n",
    "\n",
    "#Define the placeholder for the input\n",
    "\n",
    "###### YOUR CODE HERE ######\n",
    "# x = tf.placeholder(...)\n",
    "# logits,f = model(...)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add the training operations\n",
    "\n",
    "As in the gaussian example we want to define the outputs to train to and use the binary cross entropy loss function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###### YOUR CODE HERE ######"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run the training\n",
    "\n",
    "Compile the model into a session and run the training as it has been setup, as in the gaussian example.\n",
    "\n",
    "However, in this case we are going to train over mini-batches of 1024 events each step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Set up and initialise the session\n",
    "# sess = ....\n",
    "###### YOUR CODE HERE ######\n",
    "\n",
    "nEpochs=10\n",
    "batch_size=1024\n",
    "\n",
    "#Loop over epochs but this \n",
    "# I'll leave the code in place for you\n",
    "# Note that we do an inner loop on each epoch selecting a batch of events\n",
    "# We also have to make sure to format the pandas input properly with np.vstack\n",
    "for i in range(nEpochs):\n",
    "    loss_batch=[]\n",
    "    for start,end in zip(range(0,len(X_train),batch_size),\n",
    "                         range(batch_size,len(X_train)+1,batch_size)):\n",
    "        loss_,_ = sess.run([loss,minimize_loss],\n",
    "                          #feed_dict={x: X_train[start:end], labels: y_train[start:end]})\n",
    "                           feed_dict={x: np.vstack(X_train.as_matrix())[start:end], labels: np.vstack(y_train)[start:end]})\n",
    "        loss_batch.append(loss_)\n",
    "        #print loss_\n",
    "    #print loss_batch\n",
    "    loss_ = np.mean(loss_batch)\n",
    "    loss_train.append(loss_)\n",
    "    print 'Epoch',i,';',\n",
    "    print 'Train loss',loss_,';',\n",
    "    loss_ = sess.run(loss, feed_dict={x: np.vstack(X_test.as_matrix()), labels: np.vstack(y_test)})\n",
    "    loss_test.append(loss_)\n",
    "    print 'Test loss',loss_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(range(1, len(loss_train)+1), loss_train, lw=3, label=\"Training loss\")\n",
    "plt.plot(range(1, len(loss_train)+1), loss_test, lw=3, label=\"Testing loss\")\n",
    "plt.legend()\n",
    "plt.xlabel(\"Gradient step\"), plt.ylabel(\"Cross-entropy loss\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now check the performance\n",
    "\n",
    "Check for over training by plotting the decision functions for test and train.\n",
    "\n",
    "Check classification performance by plotting a ROC curve."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_train_sig = sess.run(f, feed_dict={x: X_train[y_train==1]})\n",
    "pred_train_bkgd = sess.run(f, feed_dict={x: X_train[y_train==0]})\n",
    "pred_test_sig = sess.run(f, feed_dict={x: X_test[y_test==1]})\n",
    "pred_test_bkgd = sess.run(f, feed_dict={x: X_test[y_test==0]})\n",
    "\n",
    "\n",
    "plt.hist(pred_train_sig,bins=50,range=(0.,1.),label='Train prediction sig',density=True,histtype='step')\n",
    "plt.hist(pred_train_bkgd,bins=50,range=(0.,1.),label='Train prediction bkgd',density=True,histtype='step')\n",
    "plt.hist(pred_test_sig,bins=50,range=(0.,1.),label='Train prediction sig',density=True,histtype='step',linestyle='dashed')\n",
    "plt.hist(pred_test_bkgd,bins=50,range=(0.,1.),label='Train prediction bkgd',density=True,histtype='step',linestyle='dashed')\n",
    "\n",
    "\n",
    "#plt.hist(pred_test,bins=50,range=(0,1),label='Test prediction',density=True,histtype='step')\n",
    "plt.legend(loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If everything looks good, move on to the ROC curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fpr, tpr, thresholds = roc_curve(y_test, sess.run(f, feed_dict={x:X_test}))\n",
    "roc_auc = auc(fpr, tpr)\n",
    "\n",
    "plt.plot(fpr, tpr, lw=1, label='(area = %0.2f)'%(roc_auc))\n",
    "plt.plot([0, 1], [0, 1], '--', color=(0.6, 0.6, 0.6), label='Luck')\n",
    "\n",
    "plt.xlim([-0.05, 1.05])\n",
    "plt.ylim([-0.05, 1.05])\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('Receiver operating characteristic')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.grid()   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Physics example 2: regression to calculate MT\n",
    "\n",
    "Further down, in 'Physics example 2', we look at the example of a simple regression test. \n",
    "\n",
    "Try to teach the network to learn M_T, the transverse mass of the lepton and MET. We will just do this with the signal dataset.\n",
    "\n",
    "Again start with defining a subset of required variables and splitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subset=[\n",
    "        'MT',\n",
    "        'MET','METPhi',\n",
    "        'sel_lep_pt0','sel_lep_eta0','sel_lep_phi0' #lepton 4-vector\n",
    "        ]\n",
    "\n",
    "dfR = dfFull[(dfFull.signal==1)][subset]\n",
    "\n",
    "XR=dfR.drop('MT',axis=1)\n",
    "yR=dfR['MT']#.values.reshape(len(df),1)\n",
    "\n",
    "#Split with 30% of data kept aside for testing\n",
    "XR_train,XR_test,yR_train,yR_test = model_selection.train_test_split(XR, yR, test_size=0.3, \n",
    "                                                                 random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building the neural network model\n",
    "\n",
    "Build a two layer model as above, but this time the output layer should remain linear instead of sigmoid and we reduce the number of nodes per layer. You can modify the 'model' function or rewrite the function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###### YOUR CODE HERE ######\n",
    "# xR = ...\n",
    "# logitsR,fR = model(....)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define and run the training\n",
    "\n",
    "This time we use mean squared error as a loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###### YOUR CODE HERE ######\n",
    "#labelsR = tf.placeholder(...)\n",
    "\n",
    "lossR= tf.reduce_mean(tf.losses.mean_squared_error(labels=labelsR,predictions=fR))\n",
    "#minimize_lossR = ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "###### YOUR CODE HERE ######\n",
    "# Define and initialise sessio\n",
    "\n",
    "# Run, looping over epochs and batch as in the 6th cell above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(range(1, len(lossR_train)+1), lossR_train, lw=3, label=\"Training loss\")\n",
    "plt.plot(range(1, len(lossR_train)+1), lossR_test, lw=3, label=\"Testing loss\")\n",
    "plt.legend()\n",
    "plt.xlabel(\"Gradient step\"), plt.ylabel(\"Mean squared error loss\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now make some output\n",
    "\n",
    "This time just plot the predicted vs the true M_T for the training set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predR_test = sessR.run(fR, feed_dict={xR: XR_test})\n",
    "\n",
    "plt.hist(predR_test,bins=100,range=(0,1000),label='Prediction',alpha=0.7)\n",
    "plt.hist(yR_test,bins=100,range=(0,1000),label='Truth',alpha=0.7)\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How does it look? Can you tweak the model to improve performance?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
